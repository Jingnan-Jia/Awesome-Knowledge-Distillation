# Awesome-Knowledge-Distillation
Collection of papers on Knowledge Distillation
The papers here are organized by the style described at [Knowledge Distillation: A Survey](https://arxiv.org/pdf/2006.05525.pdf)
# 1. Introduction
![image](https://user-images.githubusercontent.com/34941987/124362430-04fd0400-dc35-11eb-8d97-4b55d16d43d1.png)

# 2. Knowledge
## 2.1 Response-Based Knowledge
1. [Learning Efficient Object Detection Models with Knowledge Distillation](https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf)
2. [Fast Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.pdf)
3. [CONDITIONAL TEACHER-STUDENT LEARNING](https://arxiv.org/pdf/1904.12399.pdf)
4. [Distilling the knowledge in a neural network](https://arxiv.org/pdf/1503.02531.pdf).Hinton, G., Vinyals, O. & Dean, J. (2015).
5. [Learning with Side Information through Modality Hallucination](https://saurabhg.web.illinois.edu/pdfs/hoffman2016learning.pdf).Hoffman, J., Gupta, S. & Darrell, T. (2016).CVPR.
6. [GAN-Knowledge Distillation for one-stage Object Detection](https://arxiv.org/pdf/1906.08467.pdf). Hong, W. & Yu, J. (2019).
7. [Learning lightweight lane detection cnns by self attention distillation](https://arxiv.org/pdf/1908.00821.pdf). Hou, Y., Ma, Z., Liu, C. & Loy, CC. (2019).ICCV.
8. [Inter-Region Affinity Distillation for Road Marking Segmentation](https://arxiv.org/pdf/2004.05304.pdf). Hou, Y., Ma, Z., Liu, C., Hui, T. W., & Loy, C. C.(2020).CVPR.
9. [Mobilenets: Efficient convolutional neural networks for mobile vision applications](https://arxiv.org/pdf/1704.04861.pdf). Howard, A. G., Zhu, M., Chen, B., Kalenichenko,
D., Wang, W., Weyand, T., Andreetto, M., & Adam, H. (2017).
10. [Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing](https://arxiv.org/pdf/2004.00280.pdf). Hu, H., Xie, L., Hong, R., & Tian, Q. (2020).CVPR.
11. [Attention-guided answer distillation for machine reading comprehension](https://arxiv.org/pdf/1808.07644.pdf). Hu, M., Peng, Y., Wei, F., Huang, Z., Li, D., Yang, N.
& et al. (2018).EMNLP.
12. [Densely connected convolutional networks](https://arxiv.org/pdf/1608.06993.pdf). Huang, G., Liu, Z., Van, Der Maaten, L. & Weinberger, K. Q. (2017).CVPR.
13. [Knowledge Distillation for Sequence Model](https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1589.pdf). Huang, M., You, Y., Chen, Z., Qian, Y. & Yu, K.
(2018).Interspeech.
14. [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer](https://arxiv.org/pdf/1707.01219.pdf). Huang, Z. & Wang, N. (2017).
15. [Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection](https://arxiv.org/pdf/2010.12023.pdf). Huang, Z., Zou, Y., Bhagavatula, V., & Huang, D.
(2020).NeurIPS.
16. [Batch normalization: Accelerating deep network training by reducing internal covariate shift](https://arxiv.org/pdf/1502.03167.pdf). Ioffe, S., & Szegedy, C. (2015).ICML
17. [Learning what and where to transfer](https://arxiv.org/pdf/1905.05901.pdf). Jang, Y., Lee, H., Hwang, S. J. & Shin, J. (2019).ICML.
18. [Knowledge Distillation in Wide Neural Networks:Risk Bound, Data Efficiency and Imperfect Teacher](https://arxiv.org/pdf/2010.10090.pdf). Ji, G., & Zhu, Z. (2020).NeurIPS.
19. [Tinybert: Distilling bert for natural language understanding](https://arxiv.org/pdf/1909.10351.pdf). Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L. & et al. (2020).EMNLP.
20. [Knowledge distillation via route constrained optimization](https://arxiv.org/pdf/1904.09149.pdf). Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D., Yan, J. & Hu, X. (2019).ICCV.
21. [Towards oracle knowledge distillation with neural architecture search](https://arxiv.org/pdf/1911.13019.pdf). Kang, M., Mun, J. & Han, B. (2020).AAAI.
22. [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://arxiv.org/pdf/1802.04977.pdf). Kim, J., Park, S. & Kwak, N. (2018).NeurIPS.
23. [QKD: Quantization-aware Knowledge Distillation](https://arxiv.org/pdf/1911.12491.pdf). Kim, J., Bhalgat, Y., Lee, J., Patel, C., & Kwak, N. (2019a).
24. [Feature fusion for online mutual knowledge distillation](https://arxiv.org/pdf/1904.09058.pdf). Kim, J., Hyun, M., Chung, I. & Kwak, N. (2019b).ICPR.
25. [TRANSFERRING KNOWLEDGE TO SMALLER NETWORK WITH CLASS-DISTANCE LOSS](https://openreview.net/pdf?id=ByXrfaGFe).Kim, S. W. & Kim, H. E. (2017).ICLRW.
26. [Sequence-Level Knowledge Distillation](https://arxiv.org/pdf/1606.07947.pdf). Kim, Y., Rush & A. M. (2016).EMNLP.
27. [Few-shot learning of neural networks from scratch by pseudo example optimization](https://arxiv.org/pdf/1802.03039.pdf). Kimura, A., Ghahramani, Z., Takeuchi, K., Iwata,
T. & Ueda, N. (2018).BMVC.
28. [ADAPTIVE KNOWLEDGE DISTILLATION BASED ON ENTROPY](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054698). Kwon, K., Na, H., Lee, H., & Kim, N. S. (2020).ICASSP.
29. [Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and Residual Knowledge Distillation](https://arxiv.org/pdf/1905.10777.pdf). Kong, H., Zhao, J., Tu, X., Xing, J., Shen, S. & Feng, J. (2019).
30. [Learning multiple layers of features from tiny images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
31. [Imagenet classification with deep convolutional neural networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). Krizhevsky, A., Sutskever, I. & Hinton, G. E. (2012).NeurIPS.
32. [Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser](https://aclanthology.org/D16-1180.pdf). Kuncoro, A., Ballesteros, M., Kong, L., Dyer, C. &
Smith, N. A. (2016).EMNLP.
33. [Unsupervised multi-task adaptation using adversarial cross-task distillation](https://arxiv.org/pdf/1908.03884.pdf). Kundu, J. N., Lakkakula, N. & Babu, R. V. (2019).CVPR.
34. [Dual Policy Distillation](https://arxiv.org/pdf/2006.04061.pdf). Lai, K. H., Zha, D., Li, Y., & Hu, X. (2020).IJCAI.
35. [Self-Referenced Deep Learning](https://arxiv.org/pdf/1811.07598.pdf). Lan, X., Zhu, X., & Gong, S. (2018).ACCV.
36. [Rethinking data augmentation: Self-supervision and selfdistillation](https://openreview.net/pdf?id=SkliR1SKDS). Lee, H., Hwang, S. J. & Shin, J. (2019a).
37. [Overcoming catastrophic forgetting with unlabeled data in the wild](https://arxiv.org/pdf/1903.12648.pdf). Lee, K., Lee, K., Shin, J. & Lee, H. (2019b).ICCV.
38. [Stochasticity and Skip Connection Improve Knowledge Transfer](https://openreview.net/attachment?id=HklA93NYwS&name=original_pdf). Lee, K., Nguyen, L. T. & Shim, B. (2019c).AAAI.
39. [Graph-based knowledge distillation by multi-head attention network](https://arxiv.org/pdf/1907.02226.pdf). Lee, S. & Song, B. (2019).BMVC.
40. [Selfsupervised knowledge distillation using singular value decomposition](https://arxiv.org/pdf/1807.06819.pdf). Lee, S. H., Kim, D. H. & Song, B. C. (2018).ECCV.
