# Awesome-Knowledge-Distillation
Collection of papers on Knowledge Distillation
The papers here are organized by the style described at [Knowledge Distillation: A Survey](https://arxiv.org/pdf/2006.05525.pdf)
# 1. Introduction
![image](https://user-images.githubusercontent.com/34941987/124362430-04fd0400-dc35-11eb-8d97-4b55d16d43d1.png)

# 2. Knowledge
## 2.1 Response-Based Knowledge
1. [Learning Efficient Object Detection Models with Knowledge Distillation](https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf)
2. [Fast Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.pdf)
3. [CONDITIONAL TEACHER-STUDENT LEARNING](https://arxiv.org/pdf/1904.12399.pdf)
4. [Distilling the knowledge in a neural network](https://arxiv.org/pdf/1503.02531.pdf).Hinton, G., Vinyals, O. & Dean, J. (2015).
5. [Learning with Side Information through Modality Hallucination](https://saurabhg.web.illinois.edu/pdfs/hoffman2016learning.pdf).Hoffman, J., Gupta, S. & Darrell, T. (2016).CVPR.
6. [GAN-Knowledge Distillation for one-stage Object Detection](https://arxiv.org/pdf/1906.08467.pdf). Hong, W. & Yu, J. (2019).
7. [Learning lightweight lane detection cnns by self attention distillation](https://arxiv.org/pdf/1908.00821.pdf). Hou, Y., Ma, Z., Liu, C. & Loy, CC. (2019).ICCV.
8. [Inter-Region Affinity Distillation for Road Marking Segmentation](https://arxiv.org/pdf/2004.05304.pdf). Hou, Y., Ma, Z., Liu, C., Hui, T. W., & Loy, C. C.(2020).CVPR.
9. [Mobilenets: Efficient convolutional neural networks for mobile vision applications](https://arxiv.org/pdf/1704.04861.pdf). Howard, A. G., Zhu, M., Chen, B., Kalenichenko,
D., Wang, W., Weyand, T., Andreetto, M., & Adam, H. (2017).
10. [Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing](https://arxiv.org/pdf/2004.00280.pdf). Hu, H., Xie, L., Hong, R., & Tian, Q. (2020).CVPR.
11. [Attention-guided answer distillation for machine reading comprehension](https://arxiv.org/pdf/1808.07644.pdf). Hu, M., Peng, Y., Wei, F., Huang, Z., Li, D., Yang, N.
& et al. (2018).EMNLP.
12. [Densely connected convolutional networks](https://arxiv.org/pdf/1608.06993.pdf). Huang, G., Liu, Z., Van, Der Maaten, L. & Weinberger, K. Q. (2017).CVPR.
13. [Knowledge Distillation for Sequence Model](https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1589.pdf). Huang, M., You, Y., Chen, Z., Qian, Y. & Yu, K.
(2018).Interspeech.
14. [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer](https://arxiv.org/pdf/1707.01219.pdf). Huang, Z. & Wang, N. (2017).
15. [Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection](https://arxiv.org/pdf/2010.12023.pdf). Huang, Z., Zou, Y., Bhagavatula, V., & Huang, D.
(2020).NeurIPS.
16. [Batch normalization: Accelerating deep network training by reducing internal covariate shift](https://arxiv.org/pdf/1502.03167.pdf). Ioffe, S., & Szegedy, C. (2015).ICML
17. [Learning what and where to transfer](https://arxiv.org/pdf/1905.05901.pdf). Jang, Y., Lee, H., Hwang, S. J. & Shin, J. (2019).ICML.
18. [Knowledge Distillation in Wide Neural Networks:Risk Bound, Data Efficiency and Imperfect Teacher](https://arxiv.org/pdf/2010.10090.pdf). Ji, G., & Zhu, Z. (2020).NeurIPS.
19. [Tinybert: Distilling bert for natural language understanding](https://arxiv.org/pdf/1909.10351.pdf). Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L. & et al. (2020).EMNLP.
20. [Knowledge distillation via route constrained optimization](https://arxiv.org/pdf/1904.09149.pdf). Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D., Yan, J. & Hu, X. (2019).ICCV.
21. [Towards oracle knowledge distillation with neural architecture search](https://arxiv.org/pdf/1911.13019.pdf). Kang, M., Mun, J. & Han, B. (2020).AAAI.
22. [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://arxiv.org/pdf/1802.04977.pdf). Kim, J., Park, S. & Kwak, N. (2018).NeurIPS.
23. [QKD: Quantization-aware Knowledge Distillation](https://arxiv.org/pdf/1911.12491.pdf). Kim, J., Bhalgat, Y., Lee, J., Patel, C., & Kwak, N. (2019a).
24. [Feature fusion for online mutual knowledge distillation](https://arxiv.org/pdf/1904.09058.pdf). Kim, J., Hyun, M., Chung, I. & Kwak, N. (2019b).ICPR.
25. [TRANSFERRING KNOWLEDGE TO SMALLER NETWORK WITH CLASS-DISTANCE LOSS](https://openreview.net/pdf?id=ByXrfaGFe).Kim, S. W. & Kim, H. E. (2017).ICLRW.
26. [Sequence-Level Knowledge Distillation](https://arxiv.org/pdf/1606.07947.pdf). Kim, Y., Rush & A. M. (2016).EMNLP.
27. [Few-shot learning of neural networks from scratch by pseudo example optimization](https://arxiv.org/pdf/1802.03039.pdf). Kimura, A., Ghahramani, Z., Takeuchi, K., Iwata,
T. & Ueda, N. (2018).BMVC.
28. [ADAPTIVE KNOWLEDGE DISTILLATION BASED ON ENTROPY](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054698). Kwon, K., Na, H., Lee, H., & Kim, N. S. (2020).ICASSP.
29. [Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and Residual Knowledge Distillation](https://arxiv.org/pdf/1905.10777.pdf). Kong, H., Zhao, J., Tu, X., Xing, J., Shen, S. & Feng, J. (2019).
30. [Learning multiple layers of features from tiny images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
31. [Imagenet classification with deep convolutional neural networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). Krizhevsky, A., Sutskever, I. & Hinton, G. E. (2012).NeurIPS.
32. [Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser](https://aclanthology.org/D16-1180.pdf). Kuncoro, A., Ballesteros, M., Kong, L., Dyer, C. &
Smith, N. A. (2016).EMNLP.
33. [Unsupervised multi-task adaptation using adversarial cross-task distillation](https://arxiv.org/pdf/1908.03884.pdf). Kundu, J. N., Lakkakula, N. & Babu, R. V. (2019).CVPR.
34. [Dual Policy Distillation](https://arxiv.org/pdf/2006.04061.pdf). Lai, K. H., Zha, D., Li, Y., & Hu, X. (2020).IJCAI.
35. [Self-Referenced Deep Learning](https://arxiv.org/pdf/1811.07598.pdf). Lan, X., Zhu, X., & Gong, S. (2018).ACCV.
36. [Rethinking data augmentation: Self-supervision and selfdistillation](https://openreview.net/pdf?id=SkliR1SKDS). Lee, H., Hwang, S. J. & Shin, J. (2019a).
37. [Overcoming catastrophic forgetting with unlabeled data in the wild](https://arxiv.org/pdf/1903.12648.pdf). Lee, K., Lee, K., Shin, J. & Lee, H. (2019b).ICCV.
38. [Stochasticity and Skip Connection Improve Knowledge Transfer](https://openreview.net/attachment?id=HklA93NYwS&name=original_pdf). Lee, K., Nguyen, L. T. & Shim, B. (2019c).AAAI.
39. [Graph-based knowledge distillation by multi-head attention network](https://arxiv.org/pdf/1907.02226.pdf). Lee, S. & Song, B. (2019).BMVC.
40. [Selfsupervised knowledge distillation using singular value decomposition](https://arxiv.org/pdf/1807.06819.pdf). Lee, S. H., Kim, D. H. & Song, B. C. (2018).ECCV.
41. [Learning Light-Weight Translation Models from Deep Transformer](https://arxiv.org/pdf/2012.13866.pdf). Li, B., Wang, Z., Liu, H., Du, Q., Xiao, T., Zhang, C., & Zhu, J. (2021).AAAI.
42. [ Blockwisely Supervised Neural Architecture Search with Knowledge Distillation.](https://arxiv.org/pdf/1911.13053.pdf). Li, C., Peng, J., Yuan, L., Wang, G., Liang, X., Lin, L.,& Chang, X. (2020a).CVPR.
43. [Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts](https://proceedings.neurips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Paper.pdf). Li, G., Zhang, J., Wang, Y., Liu, C., Tan, M., Lin, Y., Zhang, W., Feng, J., & Zhang, T. (2020b).NeurIPS.
44. [Spatiotemporal knowledge distillation for efficient estimation of aerial video saliency](https://arxiv.org/pdf/1904.04992.pdf). Li, J., Fu, K., Zhao, S. & Ge, S. (2019).IEEE TIP 29:1902–1914.
45. [Gan compression: Efficient architectures for interactive conditional gans](https://arxiv.org/pdf/2003.08936.pdf). Li, M., Lin, J., Ding, Y., Liu, Z., Zhu, J. Y., & Han, S.(2020c).CVPR.
46. [Mimicking very efficient network for object detection](https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf). Li, Q., Jin, S. & Yan, J. (2017).CVPR.
47. [Few sample knowledge distillation for efficient network compression](https://arxiv.org/pdf/1812.01839.pdf). Li, T., Li, J., Liu, Z., & Zhang, C. (2020d).CVPR.
48. [Local Correlation Consistency for Knowledge Distillation](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf). Li, X., Wu, J., Fang, H., Liao, Y., Wang, F., & Qian, C. (2020e).ECCV.
49. [Learning without forgetting](https://arxiv.org/pdf/1606.09282.pdf). Li, Z. & Hoiem, D. (2017).IEEE TPAMI 40(12): 2935–2947.
50. [Ensemble distillation for robust model fusion in
federated learning](https://arxiv.org/pdf/2006.07242.pdf). Lin, T., Kong, L., Stich, S. U., & Jaggi, M. (2020).NeurIPS.
51. [Knowledge flow:Improve upon your teachers](https://arxiv.org/pdf/1904.05878.pdf). Liu, I. J., Peng, J. & Schwing, A. G. (2019a).ICLR.
52. [Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection](https://ojs.aaai.org//index.php/AAAI/article/view/4649). Liu, J., Chen, Y. & Liu, K. (2019b).AAAI.
53. [Knowledge representing:efficient, sparse representation of prior knowledge for knowledge distillation](https://arxiv.org/pdf/1911.05329.pdf). Liu, J., Wen, D., Gao, H., Tao, W., Chen, T. W., Osa, K. & et al. (2019c).CVPRW.
54. [DDFlow: Learning optical flow with unlabeled data distillation](https://arxiv.org/pdf/1902.09145.pdf). Liu, P., King, I., Lyu, M. R., & Xu, J. (2019d).AAAI.
55. [Ktan: knowledge transfer adversarial network](https://arxiv.org/pdf/1810.08126.pdf). Liu, P., Liu, W., Ma, H., Mei, T. & Seok, M. (2020a).IJCNN.
56. [Semantic-aware knowledge preservation for zero-shot sketch-based image retrieval](https://arxiv.org/pdf/1904.03208.pdf). Liu, Q., Xie, L., Wang, H., Yuille & A. L. (2019e).
.ICCV.
57. [Model compression with generative adversarial networks](https://arxiv.org/pdf/1812.02271.pdf). Liu, R., Fusi, N. & Mackey, L. (2018).
