# Awesome-Knowledge-Distillation
Collection of papers on Knowledge Distillation
The papers here are organized by the style described at [Knowledge Distillation: A Survey](https://arxiv.org/pdf/2006.05525.pdf)
# 1. Introduction
![image](https://user-images.githubusercontent.com/34941987/124362430-04fd0400-dc35-11eb-8d97-4b55d16d43d1.png)

# 2. Knowledge
## 2.1 Response-Based Knowledge
1. [Learning Efficient Object Detection Models with Knowledge Distillation](https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf)
2. [Fast Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.pdf)
3. [CONDITIONAL TEACHER-STUDENT LEARNING](https://arxiv.org/pdf/1904.12399.pdf)
4. [Distilling the knowledge in a neural network](https://arxiv.org/pdf/1503.02531.pdf).Hinton, G., Vinyals, O. & Dean, J. (2015).
5. [Learning with Side Information through Modality Hallucination](https://saurabhg.web.illinois.edu/pdfs/hoffman2016learning.pdf).Hoffman, J., Gupta, S. & Darrell, T. (2016).CVPR.
6. [GAN-Knowledge Distillation for one-stage Object Detection](https://arxiv.org/pdf/1906.08467.pdf). Hong, W. & Yu, J. (2019).
