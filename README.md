# Awesome-Knowledge-Distillation
Collection of papers on Knowledge Distillation
The papers here are organized by the style described at [Knowledge Distillation: A Survey](https://arxiv.org/pdf/2006.05525.pdf)
# Introduction
![image](https://user-images.githubusercontent.com/34941987/124362430-04fd0400-dc35-11eb-8d97-4b55d16d43d1.png)

# Paper list
1. [Spatial knowledge distillation to aid visual reasoning](https://arxiv.org/pdf/1812.03631.pdf). Aditya, S., Saha, R., Yang, Y. & Baral, C. (2019). WACV.
2. [Knowledge distillation from internal representations](https://arxiv.org/pdf/1910.03723.pdf). Aguilar, G., Ling, Y., Zhang, Y., Yao, B., Fan, X. & Guo, E. (2020). AAAI.
3. [Compressing gans using knowledge distillation](https://arxiv.org/pdf/1902.00159.pdf). Aguinaldo, A., Chiang, P. Y., Gain, A., Patil, A., Pearson, K. & Feizi, S. (2019).
4. [Variational information distillation for knowledge transfer](https://arxiv.org/pdf/1904.05835.pdf). Ahn, S., Hu, S., Damianou, A., Lawrence, N. D. & Dai, Z. (2019). CVPR.
5. [Emotion recognition in speech using crossmodal transfer in the wild](https://arxiv.org/pdf/1808.05561.pdf). Albanie, S., Nagrani, A., Vedaldi, A. & Zisserman, A. (2018). ACM MM.
6. [Learning and generalization in overparameterized neural networks going beyond two layers](https://arxiv.org/pdf/1811.04918.pdf). Allen-Zhu, Z., Li, Y., & Liang, Y. (2019). NeurIPS.
7. [Large scale distributed neural network training through online distillation](https://arxiv.org/pdf/1804.03235.pdf). Anil, R., Pereyra, G., Passos, A., Ormandi, R., Dahl, G.
E.. & Hinton, G. E. (2018). ICLR.
8. [On the optimization of deep networks: Implicit acceleration by overparameterization](https://arxiv.org/pdf/1802.06509.pdf). Arora, S., Cohen, N., & Hazan, E. (2018). ICML.
9. [On knowledge distillation from complex networks for response prediction](https://aclanthology.org/N19-1382.pdf). Arora, S., Khapra, M. M. & Ramaswamy, H. G. (2019). NAACL-HLT.
10. [Domain adaptation of dnn acoustic models using knowledge distillation](https://ieeexplore.ieee.org/document/7953145). Asami, T., Masumura, R., Yamaguchi, Y., Masataki,
H. & Aono, Y. (2017). ICASSP.
11. [N2N learning: Network to network compression via policy gradient reinforcement learning](https://arxiv.org/pdf/1709.06030.pdf). Ashok, A., Rhinehart, N., Beainy, F. & Kitani, K. M. (2018). ICLR.
12. [Ensemble knowledge distillation for learning improved and efficient networks](https://arxiv.org/pdf/1909.08097.pdf). Asif, U., Tang, J. & Harrer, S. (2020). ECAI.
13. [Do deep nets really need to be deep?](https://arxiv.org/pdf/1312.6184.pdf). Ba, J. & Caruana, R. (2014). NeurIPS.
14. [Label refinery: Improving imagenet classification through label progressio](https://arxiv.org/pdf/1805.02641.pdf). Bagherinezhad, H., Horton, M., Rastegari, M. &
Farhadi, A. (2018).
15. [Few shot network compression via cross distillation](https://arxiv.org/pdf/1911.09450.pdf). Bai, H., Wu, J., King, I. & Lyu, M. (2020). AAAI.
16. [Learn spelling from teachers: transferring knowledge from language models to sequence-to-sequence speech recognition](https://arxiv.org/pdf/1907.06017.pdf). Bai, Y., Yi, J., Tao, J., Tian, Z. &Wen, Z. (2019). Interspeech.
17. [Teacher guided architecture search](https://arxiv.org/pdf/1808.01405.pdf). Bashivan, P., Tensen, M. & DiCarlo, J. J. (2019). ICCV.
18. [Adversarial network compression](https://arxiv.org/pdf/1803.10750.pdf). Belagiannis, V., Farshad, A. & Galasso, F. (2018). ECCV.
19. [Representation learning: A review and new perspectives](https://arxiv.org/pdf/1206.5538.pdf). Bengio, Y., Courville, A., & Vincent, P. (2013). IEEE TPAMI 35(8): 1798–1828.
20. [Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings](https://arxiv.org/pdf/1911.02357.pdf). Bergmann, P., Fauser, M., Sattlegger, D., & Steger, C. (2020). CVPR.
21. [Efficient video classification using fewer frames](https://arxiv.org/pdf/1902.10640.pdf). Bhardwaj, S., Srinivasan, M. & Khapra, M. M. (2019). CVPR.
22. [Distributed Distillation for On-Device Learning](https://proceedings.neurips.cc/paper/2020/file/fef6f971605336724b5e6c0c12dc2534-Paper.pdf). Bistritz, I., Mann, A., & Bambos, N. (2020). NeurIPS.
23. [Flexible Dataset Distillation: Learn Labels Instead of Images](https://arxiv.org/pdf/2006.08572.pdf). Bohdal, O., Yang, Y., & Hospedales, T. (2020).
24. [Stochastic Precision Ensemble: Self-Knowledge Distillation for Quantized Deep Neural Networks](https://arxiv.org/pdf/2009.14502.pdf). Boo, Y., Shin, S., Choi, J., & Sung, W. (2021). AAAI.
25. [Why do Larger Models Generalize Better? A Theoretical Perspective via the XOR Problem](https://arxiv.org/pdf/1810.03037.pdf). Brutzkus, A., & Globerson, A. (2019). ICML.
26. [Model compression](https://dl.acm.org/doi/10.1145/1150402.1150464). Bucilua, C., Caruana, R. & Niculescu-Mizil, A. (2006). SIGKDD.
27. [Online Fast Adaptation and Knowledge Accumulation (OSAKA): a New Approach to Continual Learning](https://arxiv.org/pdf/2003.05856.pdf). Caccia, M., Rodriguez, P., Ostapenko, O., Normandin, F., Lin, M., Caccia, L., Laradji, I., Rish, I., Lacoste, A., Vazquez D., & Charlin, L. (2020). NeurIPS.
28. [Transferring knowledge from a rnn to a DNN](https://arxiv.org/pdf/1504.01483.pdf). Chan, W., Ke, N. R. & Lane, I. (2015).
29. [Data-Free Knowledge Distillation for Object Detection](https://openaccess.thecvf.com/content/WACV2021/papers/Chawla_Data-Free_Knowledge_Distillation_for_Object_Detection_WACV_2021_paper.pdf). Chawla, A., Yin, H., Molchanov, P., & Alvarez, J. (2021). WACV.
30. [Distilling knowledge from ensembles of neural networks for speech recognition](https://www.isca-speech.org/archive/Interspeech_2016/pdfs/1190.PDF). Chebotar, Y. & Waters, A. (2016). Interspeech.
31. [Online knowledge distillation with diverse peers](https://arxiv.org/pdf/1912.00350.pdf). Chen, D., Mei, J. P., Wang, C., Feng, Y. & Chen, C. (2020a). AAAI.
32. [Cross-Layer Distillation with Semantic Calibration](https://arxiv.org/pdf/2012.03236.pdf). Chen, D., Mei, J. P., Zhang, Y., Wang, C., Wang, Z., Feng, Y., & Chen, C. (2021).  AAAI.
33. [Learning efficient object detection models with knowledge distillation](https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf). Chen, G., Choi, W., Yu, X., Han, T., & Chandraker, M. (2017). NeurIPS.
34. [Data-Free Learning of Student Networks](https://arxiv.org/pdf/1904.01186.pdf). Chen, H., Wang, Y., Xu, C., Yang, Z., Liu, C., Shi, B., Xu, C., Xu, C., &Tian, Q. (2019a). ICCV.
35. [Learning student networks via feature embedding](https://arxiv.org/pdf/1812.06597.pdf). Chen, H., Wang, Y., Xu, C., Xu, C. & Tao, D. (2021). IEEE TNNLS 32(1): 25-35.
36. [Net2Net: ACCELERATING LEARNING VIA KNOWLEDGE TRANSFER](https://arxiv.org/pdf/1511.05641.pdf). Chen, T., Goodfellow, I. & Shlens, J. (2016). ICLR.
37. [Knowledge distillation with feature maps for image classification](https://arxiv.org/pdf/1812.00660.pdf). Chen, W. C., Chang, C. C. & Lee, C. R. (2018a). ACCV.
38. [Adversarial distillation for efficient recommendation with external knowledge](https://dl.acm.org/doi/10.1145/3281659). Chen, X., Zhang, Y., Xu, H., Qin, Z. & Zha, H. (2018b). ACM TOIS 37(1): 1–28.
39. [A two-teacher tramework for knowledge distillation](https://link.springer.com/content/pdf/10.1007%2F978-3-030-22796-8.pdf). Chen, X., Su, J. & Zhang, J. (2019b). ISNN.
40. [Darkrank: Accelerating deep metric learning via cross sample similarities transfer](https://arxiv.org/pdf/1707.01220.pdf). Chen, Y., Wang, N. & Zhang, Z. (2018c). AAAI.
41. [Distilling knowledge learned in BERT for text generation](https://arxiv.org/pdf/1911.03829.pdf). Chen, Y. C., Gan, Z., Cheng, Y., Liu, J., & Liu, J. (2020b). ACL.
42. [Crdoco: Pixel-level domain transfer with cross-domain consistency](https://arxiv.org/pdf/2001.03182.pdf). Chen, Y. C., Lin, Y. Y., Yang, M. H., Huang, J. B.
(2019c). CVPR.
43. [Lifelong Machine Learning, Second Edition Synthesis Lectures on Artificial Intelligence and Machine Learning](https://www.morganclaypool.com/doi/abs/10.2200/S00832ED1V01Y201802AIM037). Chen, Z. & Liu, B. (2018).  12(3): 1–207. 
44. [A Multi-task Mean Teacher for Semi-supervised Shadow Detection](https://openaccess.thecvf.com/content_CVPR_2020/papers/Chen_A_Multi-Task_Mean_Teacher_for_Semi-Supervised_Shadow_Detection_CVPR_2020_paper.pdf). Chen, Z., Zhu, L., Wan, L., Wang, S., Feng, W., & Heng, P. A. (2020c). CVPR.
45. [Model compression and acceleration for deep neural networks: The principles, progress, and challenges](https://ieeexplore.ieee.org/document/8253600). Cheng, Y., Wang, D., Zhou, P. & Zhang, T. (2018). IEEE Signal Proc Mag 35(1): 126–136.
46. [Explaining Knowledge Distillation by Quantifying the Knowledge](https://arxiv.org/pdf/2003.03622.pdf). Cheng, X., Rao, Z., Chen, Y., & Zhang, Q. (2020). CVPR.
47. [On the efficacy of knowledge distillation](https://arxiv.org/pdf/1910.01348.pdf). Cho, J. H. & Hariharan, B. (2019). ICCV.
48. [Xception: Deep learning with depthwise separable convolutions](https://arxiv.org/pdf/1610.02357.pdf). Chollet, F. (2017). CVPR.
49. [Feature-map-level online adversarial knowledge distillation](https://arxiv.org/pdf/2002.01775.pdf). Chung, I., Park, S., Kim, J. & Kwak, N. (2020). ICML.
50. [Bam! born-again multitask networks for natural language understanding](https://arxiv.org/pdf/1907.04829.pdf). Clark, K., Luong, M. T., Khandelwal, U., Manning,
C. D. & Le, Q. V. (2019). ACL.
51.[Binaryconnect: Training deep neural networks with binary weights during propagations](https://arxiv.org/pdf/1511.00363.pdf). Courbariaux, M., Bengio, Y. & David, J. P. (2015). NeurIPS.
52.[Moonshine: Distilling with cheap convolutions](https://arxiv.org/pdf/1711.02613.pdf). Crowley, E. J., Gray, G. & Storkey, A. J. (2018). NeurIPS.
53. [Knowledge distillation across ensembles of multilingual models or low-resource languages](https://ieeexplore.ieee.org/document/7953073). Cui, J., Kingsbury, B., Ramabhadran, B., Saon, G., Sercu, T., Audhkhasi, K. & et al. (2017).ICASSP.
54. [Knowledge Augmented Deep Neural Networks for Joint Facial Expression and Action Unit Recognition](https://proceedings.neurips.cc/paper/2020/file/a51fb975227d6640e4fe47854476d133-Paper.pdf). Cui, Z., Song, T., Wang, Y., & Ji, Q. (2020). NeurIPS.
55. [Defocus Blur Detection via Depth Distillation](https://arxiv.org/pdf/2007.08113.pdf). Cun, X., & Pun, C. M. (2020). ECCV.
56. [ImageNet: A large-scale hierarchical image database](https://ieeexplore.ieee.org/document/5206848). Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-
Fei, L. (2009). CVPR.
57. [Exploiting linear structure within convolutional networks for efficient evaluation](https://arxiv.org/pdf/1404.0736.pdf). Denton, E. L., Zaremba, W., Bruna, J., LeCun, Y. &
Fergus, R. (2014).NeurIPS.
58. [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/pdf/1810.04805.pdf) Devlin, J., Chang, M. W., Lee, K. & Toutanova, K.
(2019). NAACL-HLT.
59. [Adaptive regularization of labels](https://arxiv.org/pdf/1908.05474.pdf) Ding, Q., Wu, S., Sun, H., Guo, J. & Xia, ST. (2019).
60. [Compact trilinear interaction for visual question answering](https://arxiv.org/pdf/1909.11874.pdf) Do, T., Do, T. T., Tran, H., Tjiputra, E. & Tran, Q.
D. (2019). ICCV.
61. [Teacher supervises students how to learn from partially labeled images for facial landmark detection](https://arxiv.org/pdf/1908.02116.pdf) Dong, X. & Yang, Y. (2019). ICCV.
62. [Unpaired multi-modal segmentation via knowledge distillation](https://arxiv.org/pdf/2001.03111.pdf) Dou, Q., Liu, Q., Heng, P. A., & Glocker, B. (2020). IEEE TMI 
63. [Agree to Disagree: Adaptive Ensemble Knowledge Distillation in Gradient Space](https://proceedings.neurips.cc/paper/2020/file/91c77393975889bd08f301c9e13a44b7-Paper.pdf) Du, S., You, S., Li, X., Wu, J., Wang, F., Qian, C., & Zhang, C. (2020). NeurIPS.
64. [ShrinkTeaNet: Million-scale lightweight face recognition via shrinking teacher-student networks](https://arxiv.org/pdf/1905.10620.pdf) Duong, C. N., Luu, K., Quach, K. G. & Le, N. (2019.)
65. [Fast, Accurate, and Simple Models for Tabular Data via Augmented Distillation](https://arxiv.org/pdf/2006.14284.pdf) Fakoor, R., Mueller, J. W., Erickson, N., Chaudhari, P., & Smola, A. J. (2020). NeurIPS.
66. [Transferring knowledge across learning processes](https://arxiv.org/pdf/1812.01054.pdf) Flennerhag, S., Moreno, P. G., Lawrence, N. D. & Damianou, A. (2019). ICLR.
67. [Ensemble distillation for neural machine translation](https://arxiv.org/pdf/1702.01802.pdf) Freitag, M., Al-Onaizan, Y. & Sankaran, B. (2017).
68. Fu, H., Zhou, S., Yang, Q., Tang, J., Liu, G., Liu, K., & Li, X. (2021). LRC-BERT: Latent representation Contrastive Knowledge Distillation for Natural Language Understanding. In: AAAI.
Fukuda, T., Suzuki, M., Kurata, G., Thomas, S., Cui, J. & Ramabhadran, B. (2017). Efficient knowledge
distillation from an ensemble of teachers. In:
Interspeech.
Furlanello, T., Lipton, Z., Tschannen, M., Itti, L. & Anandkumar, A. (2018). Born again neural networks.
In: ICML.
Gao, L., Mi, H., Zhu, B., Feng, D., Li, Y. & Peng, Y. (2019). An adversarial feature distillation method for
audio classification. IEEE Access 7: 105319–105330.
Gao, M., Wang, Y., & Wan, L. (2021). Residual Error Based Knowledge Distillation. Neurocomputing 433:
154-161.
Gao, Z., Chung, J., Abdelrazek, M., Leung, S., Hau,W. K., Xian, Z., Zhang, H., & Li, S. (2020). Privileged
modality distillation for vessel border detection in
intracoronary imaging. IEEE TMI 39(5): 1524-1534.
Garcia, N. C., Morerio, P. & Murino, V. (2018). Modality distillation with multiple stream networks
for action recognition. In: ECCV.
Ge, S., Zhao, S., Li, C. & Li, J. (2018). Low-resolution face recognition in the wild via selective knowledge
distillation. IEEE TIP 28(4):2051–2062.
Ge, S., Zhao, S., Li, C., Zhang, Y., & Li, J. (2020). Efficient Low-Resolution Face Recognition via Bridge Distillation. IEEE TIP 29: 6898-6908.
Ghorbani, S., Bulut, A. E. & Hansen, J. H. (2018). Advancing multi-accented lstm-ctc speech recognition using a domain specific student-teacher
learning paradigm. In: SLTW.
Gil, Y., Chai, Y., Gorodissky, O. & Berant, J. (2019). White-to-black: Efficient distillation of black-box adversarial attacks. In: NAACL-HLT.
Goldblum, M., Fowl, L., Feizi, S. & Goldstein, T.
(2020). Adversarially robust distillation. In: AAAI.
Gong, C., Chang, X., Fang, M. & Yang, J. (2018). Teaching semi-supervised classifier via generalized distillation. In: IJCAI.
Gong, C., Tao, D., Liu, W., Liu, L., & Yang, J. (2017). Label propagation via teaching-to-learn and learningto-teach. TNNLS 28(6): 1452–1465.
Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., & Bengio,
Y. (2014). Generative adversarial nets. In: NeurIPS.
Gordon, M. A. & Duh, K. (2019). Explaining sequencelevel knowledge distillation as data-augmentation
for neural machine translation. arXiv preprint
arXiv:1912.03334.
Gu, J., & Tresp, V. (2020). Search for Better Students to Learn Distilled Knowledge. In: ECAI.
Guan, Y., Zhao, P., Wang, B., Zhang, Y., Yao, C.,
Bian, K., & Tang, J. (2020). Differentiable Feature Aggregation Search for Knowledge Distillation. In:
ECCV.
Guo, Q., Wang, X., Wu, Y., Yu, Z., Liang, D., Hu, X., & Luo, P. (2020). Online Knowledge Distillation via
Collaborative Learning. In: CVPR.
Gupta, S., Hoffman, J. & Malik, J. (2016). Cross modal distillation for supervision transfer. In: CVPR.
Hahn, S. & Choi, H. (2019). Self-knowledge distillation
in natural language processing. In: RANLP.
Haidar, M. A. & Rezagholizadeh, M. (2019). Textkdgan: Text generation using knowledge distillation
and generative adversarial networks. In: Canadian
Conference on Artificial Intelligence.
Han, S., Pool, J., Tran, J. & Dally, W. (2015). Learning both weights and connections for efficient neural
network. In: NeurIPS.
Hao, W. & Zhang, Z. (2019). Spatiotemporal distilled dense-connectivity network for video action recognition.
Pattern Recogn 92: 13–24.
Haroush, M., Hubara, I., Hoffer, E., & Soudry, D. (2020). The knowledge within: Methods for data-free
model compression. In: CVPR.
He, C., Annavaram, M., & Avestimehr, S. (2020a).
Group Knowledge Transfer: Federated Learning of
Large CNNs at the Edge. In: NeurIPS.
He, F., Liu, T., & Tao, D. (2020b). Why resnet works?
residuals generalize. IEEE TNNLS 31(12): 5349–
5362.
He, K., Zhang, X., Ren, S. & Sun, J. (2016) Deep
residual learning for image recognition. In: CVPR.
He, T., Shen, C., Tian, Z., Gong, D., Sun, C. & Yan, Y.
(2019). Knowledge adaptation for efficient semantic
segmentation. In: CVPR.
Heo, B., Kim, J., Yun, S., Park, H., Kwak, N., & Choi, J. Y. (2019a). A comprehensive overhaul of feature
distillation. In: ICCV.
Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019b).
Knowledge distillation with adversarial samples supporting decision boundary. In: AAAI.
Heo, B., Lee, M., Yun, S. & Choi, J. Y. (2019c).
Knowledge transfer via distillation of activation
boundaries formed by hidden neurons. In: AAAI.
Hinton, G., Vinyals, O. & Dean, J. (2015). Distilling
the knowledge in a neural network. arXiv preprint
arXiv:1503.02531.
Hoffman, J., Gupta, S. & Darrell, T. (2016). Learning
with side information through modality hallucination.
In: CVPR.

69. [Learning Efficient Object Detection Models with Knowledge Distillation](https://papers.nips.cc/paper/2017/file/e1e32e235eee1f970470a3a6658dfdd5-Paper.pdf)
70. [Fast Human Pose Estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.pdf)
71. [CONDITIONAL TEACHER-STUDENT LEARNING](https://arxiv.org/pdf/1904.12399.pdf)
72. [Distilling the knowledge in a neural network](https://arxiv.org/pdf/1503.02531.pdf).Hinton, G., Vinyals, O. & Dean, J. (2015).
73. [Learning with Side Information through Modality Hallucination](https://saurabhg.web.illinois.edu/pdfs/hoffman2016learning.pdf).Hoffman, J., Gupta, S. & Darrell, T. (2016). CVPR.
74. [GAN-Knowledge Distillation for one-stage Object Detection](https://arxiv.org/pdf/1906.08467.pdf). Hong, W. & Yu, J. (2019).
75. [Learning lightweight lane detection cnns by self attention distillation](https://arxiv.org/pdf/1908.00821.pdf). Hou, Y., Ma, Z., Liu, C. & Loy, CC. (2019). ICCV.
76. [Inter-Region Affinity Distillation for Road Marking Segmentation](https://arxiv.org/pdf/2004.05304.pdf). Hou, Y., Ma, Z., Liu, C., Hui, T. W., & Loy, C. C.(2020). CVPR.
77. [Mobilenets: Efficient convolutional neural networks for mobile vision applications](https://arxiv.org/pdf/1704.04861.pdf). Howard, A. G., Zhu, M., Chen, B., Kalenichenko,
D., Wang, W., Weyand, T., Andreetto, M., & Adam, H. (2017).
10. [Creating Something from Nothing: Unsupervised Knowledge Distillation for Cross-Modal Hashing](https://arxiv.org/pdf/2004.00280.pdf). Hu, H., Xie, L., Hong, R., & Tian, Q. (2020). CVPR.
11. [Attention-guided answer distillation for machine reading comprehension](https://arxiv.org/pdf/1808.07644.pdf). Hu, M., Peng, Y., Wei, F., Huang, Z., Li, D., Yang, N.
& et al. (2018). EMNLP.
12. [Densely connected convolutional networks](https://arxiv.org/pdf/1608.06993.pdf). Huang, G., Liu, Z., Van, Der Maaten, L. & Weinberger, K. Q. (2017). CVPR.
13. [Knowledge Distillation for Sequence Model](https://www.isca-speech.org/archive/Interspeech_2018/pdfs/1589.pdf). Huang, M., You, Y., Chen, Z., Qian, Y. & Yu, K.
(2018). Interspeech.
14. [Like What You Like: Knowledge Distill via Neuron Selectivity Transfer](https://arxiv.org/pdf/1707.01219.pdf). Huang, Z. & Wang, N. (2017).
15. [Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection](https://arxiv.org/pdf/2010.12023.pdf). Huang, Z., Zou, Y., Bhagavatula, V., & Huang, D.
(2020). NeurIPS.
16. [Batch normalization: Accelerating deep network training by reducing internal covariate shift](https://arxiv.org/pdf/1502.03167.pdf). Ioffe, S., & Szegedy, C. (2015). ICML
17. [Learning what and where to transfer](https://arxiv.org/pdf/1905.05901.pdf). Jang, Y., Lee, H., Hwang, S. J. & Shin, J. (2019). ICML.
18. [Knowledge Distillation in Wide Neural Networks:Risk Bound, Data Efficiency and Imperfect Teacher](https://arxiv.org/pdf/2010.10090.pdf). Ji, G., & Zhu, Z. (2020). NeurIPS.
19. [Tinybert: Distilling bert for natural language understanding](https://arxiv.org/pdf/1909.10351.pdf). Jiao, X., Yin, Y., Shang, L., Jiang, X., Chen, X., Li, L. & et al. (2020). EMNLP.
20. [Knowledge distillation via route constrained optimization](https://arxiv.org/pdf/1904.09149.pdf). Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D., Yan, J. & Hu, X. (2019). ICCV.
21. [Towards oracle knowledge distillation with neural architecture search](https://arxiv.org/pdf/1911.13019.pdf). Kang, M., Mun, J. & Han, B. (2020). AAAI.
22. [Paraphrasing Complex Network: Network Compression via Factor Transfer](https://arxiv.org/pdf/1802.04977.pdf). Kim, J., Park, S. & Kwak, N. (2018). NeurIPS.
23. [QKD: Quantization-aware Knowledge Distillation](https://arxiv.org/pdf/1911.12491.pdf). Kim, J., Bhalgat, Y., Lee, J., Patel, C., & Kwak, N. (2019a).
24. [Feature fusion for online mutual knowledge distillation](https://arxiv.org/pdf/1904.09058.pdf). Kim, J., Hyun, M., Chung, I. & Kwak, N. (2019b). ICPR.
25. [TRANSFERRING KNOWLEDGE TO SMALLER NETWORK WITH CLASS-DISTANCE LOSS](https://openreview.net/pdf?id=ByXrfaGFe).Kim, S. W. & Kim, H. E. (2017). ICLRW.
26. [Sequence-Level Knowledge Distillation](https://arxiv.org/pdf/1606.07947.pdf). Kim, Y., Rush & A. M. (2016). EMNLP.
27. [Few-shot learning of neural networks from scratch by pseudo example optimization](https://arxiv.org/pdf/1802.03039.pdf). Kimura, A., Ghahramani, Z., Takeuchi, K., Iwata,
T. & Ueda, N. (2018). BMVC.
28. [ADAPTIVE KNOWLEDGE DISTILLATION BASED ON ENTROPY](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9054698). Kwon, K., Na, H., Lee, H., & Kim, N. S. (2020). ICASSP.
29. [Cross-Resolution Face Recognition via Prior-Aided Face Hallucination and Residual Knowledge Distillation](https://arxiv.org/pdf/1905.10777.pdf). Kong, H., Zhao, J., Tu, X., Xing, J., Shen, S. & Feng, J. (2019).
30. [Learning multiple layers of features from tiny images](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).
31. [Imagenet classification with deep convolutional neural networks](https://papers.nips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf). Krizhevsky, A., Sutskever, I. & Hinton, G. E. (2012). NeurIPS.
32. [Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser](https://aclanthology.org/D16-1180.pdf). Kuncoro, A., Ballesteros, M., Kong, L., Dyer, C. &
Smith, N. A. (2016). EMNLP.
33. [Unsupervised multi-task adaptation using adversarial cross-task distillation](https://arxiv.org/pdf/1908.03884.pdf). Kundu, J. N., Lakkakula, N. & Babu, R. V. (2019). CVPR.
34. [Dual Policy Distillation](https://arxiv.org/pdf/2006.04061.pdf). Lai, K. H., Zha, D., Li, Y., & Hu, X. (2020). IJCAI.
35. [Self-Referenced Deep Learning](https://arxiv.org/pdf/1811.07598.pdf). Lan, X., Zhu, X., & Gong, S. (2018). ACCV.
36. [Rethinking data augmentation: Self-supervision and selfdistillation](https://openreview.net/pdf?id=SkliR1SKDS). Lee, H., Hwang, S. J. & Shin, J. (2019a).
37. [Overcoming catastrophic forgetting with unlabeled data in the wild](https://arxiv.org/pdf/1903.12648.pdf). Lee, K., Lee, K., Shin, J. & Lee, H. (2019b). ICCV.
38. [Stochasticity and Skip Connection Improve Knowledge Transfer](https://openreview.net/attachment?id=HklA93NYwS&name=original_pdf). Lee, K., Nguyen, L. T. & Shim, B. (2019c). AAAI.
39. [Graph-based knowledge distillation by multi-head attention network](https://arxiv.org/pdf/1907.02226.pdf). Lee, S. & Song, B. (2019). BMVC.
40. [Selfsupervised knowledge distillation using singular value decomposition](https://arxiv.org/pdf/1807.06819.pdf). Lee, S. H., Kim, D. H. & Song, B. C. (2018). ECCV.
41. [Learning Light-Weight Translation Models from Deep Transformer](https://arxiv.org/pdf/2012.13866.pdf). Li, B., Wang, Z., Liu, H., Du, Q., Xiao, T., Zhang, C., & Zhu, J. (2021). AAAI.
42. [ Blockwisely Supervised Neural Architecture Search with Knowledge Distillation.](https://arxiv.org/pdf/1911.13053.pdf). Li, C., Peng, J., Yuan, L., Wang, G., Liang, X., Lin, L.,& Chang, X. (2020a). CVPR.
43. [Residual Distillation: Towards Portable Deep Neural Networks without Shortcuts](https://proceedings.neurips.cc/paper/2020/file/657b96f0592803e25a4f07166fff289a-Paper.pdf). Li, G., Zhang, J., Wang, Y., Liu, C., Tan, M., Lin, Y., Zhang, W., Feng, J., & Zhang, T. (2020b). NeurIPS.
44. [Spatiotemporal knowledge distillation for efficient estimation of aerial video saliency](https://arxiv.org/pdf/1904.04992.pdf). Li, J., Fu, K., Zhao, S. & Ge, S. (2019). IEEE TIP 29:1902–1914.
45. [Gan compression: Efficient architectures for interactive conditional gans](https://arxiv.org/pdf/2003.08936.pdf). Li, M., Lin, J., Ding, Y., Liu, Z., Zhu, J. Y., & Han, S.(2020c). CVPR.
46. [Mimicking very efficient network for object detection](https://openaccess.thecvf.com/content_cvpr_2017/papers/Li_Mimicking_Very_Efficient_CVPR_2017_paper.pdf). Li, Q., Jin, S. & Yan, J. (2017). CVPR.
47. [Few sample knowledge distillation for efficient network compression](https://arxiv.org/pdf/1812.01839.pdf). Li, T., Li, J., Liu, Z., & Zhang, C. (2020d). CVPR.
48. [Local Correlation Consistency for Knowledge Distillation](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123570018.pdf). Li, X., Wu, J., Fang, H., Liao, Y., Wang, F., & Qian, C. (2020e). ECCV.
49. [Learning without forgetting](https://arxiv.org/pdf/1606.09282.pdf). Li, Z. & Hoiem, D. (2017).IEEE TPAMI 40(12): 2935–2947.
50. [Ensemble distillation for robust model fusion in
federated learning](https://arxiv.org/pdf/2006.07242.pdf). Lin, T., Kong, L., Stich, S. U., & Jaggi, M. (2020). NeurIPS.
51. [Knowledge flow:Improve upon your teachers](https://arxiv.org/pdf/1904.05878.pdf). Liu, I. J., Peng, J. & Schwing, A. G. (2019a). ICLR.
52. [Exploiting the ground-truth: An adversarial imitation based knowledge distillation approach for event detection](https://ojs.aaai.org//index.php/AAAI/article/view/4649). Liu, J., Chen, Y. & Liu, K. (2019b). AAAI.
53. [Knowledge representing:efficient, sparse representation of prior knowledge for knowledge distillation](https://arxiv.org/pdf/1911.05329.pdf). Liu, J., Wen, D., Gao, H., Tao, W., Chen, T. W., Osa, K. & et al. (2019c). CVPRW.
54. [DDFlow: Learning optical flow with unlabeled data distillation](https://arxiv.org/pdf/1902.09145.pdf). Liu, P., King, I., Lyu, M. R., & Xu, J. (2019d). AAAI.
55. [Ktan: knowledge transfer adversarial network](https://arxiv.org/pdf/1810.08126.pdf). Liu, P., Liu, W., Ma, H., Mei, T. & Seok, M. (2020a). IJCNN.
56. [Semantic-aware knowledge preservation for zero-shot sketch-based image retrieval](https://arxiv.org/pdf/1904.03208.pdf). Liu, Q., Xie, L., Wang, H., Yuille & A. L. (2019e).
. ICCV.
57. [Model compression with generative adversarial networks](https://arxiv.org/pdf/1812.02271.pdf). Liu, R., Fusi, N. & Mackey, L. (2018).
58. [FastBERT: a self-distilling BERT with Adaptive Inference Time](https://arxiv.org/pdf/2004.02178.pdf). Liu, W., Zhou, P., Zhao, Z., Wang, Z., Deng, H., & Ju,Q. (2020b). ACL.
59. [Improving the interpretability of deep neural networks with knowledge distillation](https://arxiv.org/pdf/1812.10924.pdf). Liu, X., Wang, X. & Matwin, S. (2018b). ICDMW.
60. [Improving multi-task deep neural networks via knowledge distillation for natural language understanding](https://arxiv.org/pdf/1904.09482.pdf). Liu, X., He, P., Chen, W. & Gao, J. (2019f).
61. [Knowledge distillation via instance relationship graph](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Knowledge_Distillation_via_Instance_Relationship_Graph_CVPR_2019_paper.pdf). Liu, Y., Cao, J., Li, B., Yuan, C., Hu, W., Li, Y. & Duan, Y. (2019g). CVPR.
62. [Structured knowledge distillation for semantic segmentation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Liu_Structured_Knowledge_Distillation_for_Semantic_Segmentation_CVPR_2019_paper.pdf). Liu, Y., Chen, K., Liu, C., Qin, Z., Luo, Z. & Wang, J. (2019h). CVPR.
63. [Search to distill: Pearls are everywhere but not the eyes](https://arxiv.org/pdf/1911.09074.pdf). Liu, Y., Jia, X., Tan, M., Vemulapalli, R., Zhu, Y., Green, B. & et al. (2019i). CVPR.
64. [Adaptive multi-teacher multi-level knowledge distillation](https://arxiv.org/pdf/2103.04062.pdf). Liu, Y., Zhang, W., & Wang, J. (2020c). Neu-rocomputing 415: 106-113.
65. [Data-free knowledge distillation for deep neural networks](https://arxiv.org/pdf/1710.07535.pdf). Lopes, R. G., Fenu, S. & Starner, T. (2017). NeurIPS.
66. [Unifying distillation and privileged information](https://arxiv.org/pdf/1511.03643.pdf). Lopez-Paz, D., Bottou, L., Sch¨olkopf, B. & Vapnik, V. (2016). ICLR.
67. [Knowledge distillation for small-footprint highway networks](https://arxiv.org/pdf/1608.00892.pdf). Lu, L., Guo, M. & Renals, S. (2017). ICASSP.
68. [Face model compression by distilling knowledge from neurons](https://ojs.aaai.org/index.php/AAAI/article/view/10449/10308). Luo, P., Zhu, Z., Liu, Z., Wang, X. & Tang, X. (2016). AAAI.
69. [Collaboration by Competition: Selfcoordinated Knowledge Amalgamation for Multitalent Student Learning](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123510630.pdf). Luo, S., Pan, W., Wang, X., Wang, D., Tang, H., & Song, M. (2020). ECCV.
70. [Knowledge amalgamation from heterogeneous networks by common feature learning](https://arxiv.org/pdf/1906.10546.pdf). Luo, S., Wang, X., Fang, G., Hu, Y., Tao, D., &
Song, M. (2019). IJCAI.
71. [Graph distillation for action detection with privileged modalities](https://arxiv.org/pdf/1712.00108.pdf). Luo, Z., Hsieh, J. T., Jiang, L., Carlos Niebles, J.& Fei-
Fei, L. (2018). ECCV.
72. [Improving neural architecture search image classifiers via ensemble learning](https://arxiv.org/pdf/1903.06236.pdf). Macko, V., Weill, C., Mazzawi, H. & Gonzalvo, J. (2019). NeurIPS Workshop.
73. [Graph representation learning via multi-task knowledge distillation](https://arxiv.org/pdf/1911.05700.pdf). Ma, J., & Mei, Q. (2019).
74. [Shufflenet v2: Practical guidelines for efficient cnn architecture design](https://arxiv.org/pdf/1807.11164.pdf). Ma, N., Zhang, X., Zheng, H. T., & Sun, J. (2018). ECCV
75. [Conditional teacher-student learning](https://arxiv.org/pdf/1904.12399.pdf). Meng, Z., Li, J., Zhao, Y. & Gong, Y. (2019). ICASSP.
76. [Zero-shot Knowledge Transfer via Adversarial Belief Matching](https://arxiv.org/pdf/1905.09768.pdf). Micaelli, P. & Storkey, A. J. (2019). NeurIPS.
77. [Knowledge transfer graph for deep collaborative learning](https://arxiv.org/pdf/1909.04286.pdf). Minami, S., Hirakawa, T., Yamashita, T. & Fujiyoshi, H. (2019).
78. [Improved knowledge distillation via teacher assistant](https://arxiv.org/pdf/1902.03393.pdf). Mirzadeh, S. I., Farajtabar,M., Li, A. & Ghasemzadeh, H. (2020). AAAI.
79. [Apprentice: Using knowledge distillation techniques to improve lowprecision network accuracy](https://arxiv.org/pdf/1711.05852.pdf). Mishra, A. & Marr, D. (2018). ICLR.
80. [Self-distillation amplifies regularization in hilbert space](https://arxiv.org/pdf/2002.05715.pdf). Mobahi, H., Farajtabar, M., & Bartlett, P. L. (2020). NeurIPS.
81. [Distilling word embeddings: An encoding approach](https://arxiv.org/pdf/1506.04488.pdf). Mou, L., Jia, R., Xu, Y., Li, G., Zhang, L. & Jin, Z. (2016). CIKM.
82. [Cogni-net: Cognitive feature learning through deep visual perception](https://arxiv.org/pdf/1811.00201.pdf). Mukherjee, P., Das, A., Bhunia, A. K. & Roy, P. P. (2019). ICIP.
83. [Online model distillation for efficient video inference](https://arxiv.org/pdf/1812.02699.pdf). Mullapudi, R. T., Chen, S., Zhang, K., Ramanan, D. & Fatahalian, K. (2019). ICCV.
84. [When does label smoothing help?](https://arxiv.org/pdf/1906.02629.pdf). Muller, R., Kornblith, S. & Hinton, G. E. (2019). NeurIPS.
85. [Learning to specialize with knowledge distillation for visual question answering](https://proceedings.neurips.cc/paper/2018/file/0f2818101a7ac4b96ceeba38de4b934c-Paper.pdf). Mun, J., Lee, K., Shin, J. & Han, B. (2018). NeurIPS.
86. [Knowledge distillation for end-to-end person search](https://arxiv.org/pdf/1909.01058.pdf). Munjal, B., Galasso, F. & Amin, S. (2019). BMVC.
87. [Knowledge Distillation for Bilingual Dictionary Induction](https://aclanthology.org/D17-1264.pdf). Nakashole, N. & Flauger, R. (2017). EMNLP.
88. [Effectiveness of Arbitrary Transfer Sets for Data-free Knowledge Distillation](https://arxiv.org/pdf/2011.09113.pdf). Nayak, G. K.,Mopuri, K. R., & Chakraborty, A. (2021).  WACV.
89. [Zero-shot knowledge distillation in deep networks](https://arxiv.org/pdf/1905.08114.pdf). Nayak, G. K., Mopuri, K. R., Shaj, V., Babu, R. V. & Chakraborty, A. (2019). ICML.
90. [Teacherstudent training for text-independent speaker recognition](https://ieeexplore.ieee.org/document/8639564). Ng, R. W., Liu, X. & Swietojanski, P. (2018). SLTW.
91. [Dynamic kernel distillation for efficient pose estimation in videos](https://arxiv.org/pdf/1908.09216.pdf). Nie, X., Li, Y., Luo, L., Zhang, N. & Feng, J. (2019). ICCV.
92. [Boosting self-supervised learning via knowledge transfer](https://arxiv.org/pdf/1805.00385.pdf). Noroozi, M., Vinjimoor, A., Favaro, P. & Pirsiavash, H. (2018). CVPR.
93. [Deep net triage: Analyzing the importance of network layers via structural compression](https://arxiv.org/pdf/1801.04651.pdf). Nowak, T. S. & Corso, J. J. (2018). 
94. [Parallel wavenet: Fast high-fidelity speech synthesis](https://arxiv.org/pdf/1711.10433.pdf). Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K. & et al. (2018). ICML.
95. [Spatio-Temporal Graph for Video Captioning with Knowledge Distillation](https://arxiv.org/pdf/2003.13942.pdf). Pan, B., Cai, H., Huang, D. A., Lee, K. H., Gaidon, A., Adeli, E., & Niebles, J. C. (2020). CVPR
96. [A novel enhanced collaborative autoencoder with knowledge distillation for top-n recommender systems](https://www.sciencedirect.com/science/article/abs/pii/S0925231218314796). Pan, Y., He, F. & Yu, H. (2019). Neurocomputing 332: 137–148.
97. [Semi-supervised knowledge transfer for deep learning from private training data](https://arxiv.org/pdf/1610.05755.pdf). Papernot, N., Abadi, M., Erlingsson, U., Goodfellow,
I. & Talwar, K. (2017). ICLR
98. [Distillation as a defense to adversarial perturbations against deep neural networks](https://arxiv.org/pdf/1511.04508.pdf). Papernot, N., McDaniel, P., Wu, X., Jha, S. & Swami, A. (2016). IEEE SP.
99. [Feature-level Ensemble Knowledge Distillation for Aggregating Knowledge from Multiple Networks](https://ecai2020.eu/papers/405_paper.pdf). Park, S. & Kwak, N. (2020). ECAI.
100. [Relational knowledge distillation](https://arxiv.org/pdf/1904.05068.pdf). Park,W., Kim, D., Lu, Y. & Cho, M. (2019). CVPR.
101. [ALP-KD: Attention-Based Layer Projection for Knowledge Distillation](https://arxiv.org/pdf/2012.14022.pdf). Passban, P., Wu, Y., Rezagholizadeh, M., & Liu, Q. (2021). AAAI.
102. [Learning deep representations with probabilistic knowledge transfer](https://arxiv.org/pdf/1803.10837.pdf). Passalis, N. & Tefas, A. (2018). ECCV.
103. [Probabilistic Knowledge Transfer for Lightweight Deep Representation Learning](https://ieeexplore.ieee.org/document/9104915). Passalis, N., Tzelepi, M., & Tefas, A. (2020a).TNNLS. 
104. [Heterogeneous Knowledge Distillation using Information Flow Modeling](https://arxiv.org/pdf/2005.00727.pdf). Passalis, N., Tzelepi,M., & Tefas, A. (2020b). CVPR.
105. [Correlation congruence for knowledge distillation](https://arxiv.org/pdf/1904.01802.pdf). Peng, B., Jin, X., Liu, J., Li, D., Wu, Y., Liu, Y. & et al. (2019a). ICCV.
106. [Cream of the Crop: Distilling Prioritized Paths For One-Shot Neural Architecture Search](https://arxiv.org/pdf/2010.15821.pdf). Peng, H., Du, H., Yu, H., Li, Q., Liao, J., & Fu, J. (2020). NeurIPS.
107. [Few-shot image recognition with knowledge transfer](https://openaccess.thecvf.com/content_ICCV_2019/papers/Peng_Few-Shot_Image_Recognition_With_Knowledge_Transfer_ICCV_2019_paper.pdf). Peng, Z., Li, Z., Zhang, J., Li, Y., Qi, G. J. & Tang, J. (2019b). ICCV.
108. [Audio-visual model distillation using acoustic images](https://arxiv.org/pdf/1904.07933.pdf). Perez, A., Sanguineti, V., Morerio, P. & Murino, V. (2020). Audio-visual model distillation using acoustic images. WACV.
109. [Towards understanding knowledge distillation](http://proceedings.mlr.press/v97/phuong19a/phuong19a.pdf). Phuong, M. & Lampert, C. H. (2019a). ICML.
110. [ Distillationbased training for multi-exit architectures](https://openaccess.thecvf.com/content_ICCV_2019/papers/Phuong_Distillation-Based_Training_for_Multi-Exit_Architectures_ICCV_2019_paper.pdf). Phuong, M., & Lampert, C. H. (2019b). ICCV.
111. [Refine and distill: Exploiting cycle-inconsistency and knowledge distillation for unsupervised monocular depth estimation](https://arxiv.org/pdf/1903.04202.pdf). Pilzer, A., Lathuiliere, S., Sebe, N. & Ricci, E. (2019). CVPR.
112. [Model compression via distillation and quantization](https://arxiv.org/pdf/1802.05668.pdf). Polino, A., Pascanu, R. & Alistarh, D. (2018). ICLR.
113. [Wise teachers train better dnn acoustic models](https://link.springer.com/content/pdf/10.1186/s13636-016-0088-7.pdf). Price, R., Iso, K. & Shinoda, K. (2016). . EURASIP Journal on Audio, Speech, and Music Processing 2016(1):10.
114. [Data distillation: Towards omnisupervised learning](https://arxiv.org/pdf/1712.04440.pdf). Radosavovic, I., Dollar, P., Girshick, R., Gkioxari, G., & He, K. (2018). CVPR.
115. [Designing network design spaces](https://arxiv.org/pdf/2003.13678.pdf). Radosavovic, I., Kosaraju, R. P., Girshick, R., He, K., & Dollar P. (2020). CVPR.
116. [Cross-modality distillation: A case for conditional generative adversarial networks](https://arxiv.org/pdf/1807.07682.pdf). Roheda, S., Riggan, B. S., Krim, H. & Dai, L. (2018). ICASSP.
117. [Fitnets: Hints for thin deep nets](https://arxiv.org/pdf/1412.6550.pdf). Romero, A., Ballas, N., Kahou, S. E., Chassang, A., Gatta, C., & Bengio, Y. (2015). ICLR.
118. [Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients](https://arxiv.org/pdf/1711.09404.pdf). Ross, A. S. & Doshi-Velez, F. (2018). AAAI.
119. [Knowledge adaptation: Teaching to adapt](https://arxiv.org/pdf/1702.02052.pdf). Ruder, S., Ghaffari, P. & Breslin, J. G. (2017).
120. [ Mobilenetv2: Inverted residuals and linear bottlenecks](https://arxiv.org/pdf/1801.04381.pdf). Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., & Chen, L. C. (2018). CVPR.
121. [Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter](https://arxiv.org/pdf/1910.01108.pdf). Sanh, V., Debut, L., Chaumond, J. & Wolf, T. (2019). 
122. [Distilling knowledge from a deep pose regressor network](https://arxiv.org/pdf/1908.00858.pdf). Saputra, M. R. U., de Gusmao, P. P., Almalioglu, Y., Markham, A. & Trigoni, N. (2019). ICCV.
123. [Deep model compression: Distilling knowledge from noisy teachers](https://arxiv.org/pdf/1610.09650.pdf). Sau, B. B. & Balasubramanian, V. N. (2016).
124. [Federated Knowledge Distillation](https://arxiv.org/pdf/2011.02367.pdf). Seo, H., Park, J., Oh, S., Bennis, M., & Kim, S. L. (2020).  
125. [Knowledge distillation in document retrieval](https://arxiv.org/pdf/1911.11065.pdf). Shakeri, S., Sethy, A. & Cheng, C. (2019).
126. [Amalgamating knowledge towards comprehensive classification](https://arxiv.org/pdf/1811.02796.pdf). Shen, C., Wang, X., Song, J., Sun, L., & Song, M. (2019a). AAAI.
127. [Progressive Network Grafting for Few-Shot Knowledge Distillation](https://arxiv.org/pdf/2012.04915.pdf). Shen, C., Wang, X., Yin, Y., Song, J., Luo, S., & Song, M. (2021). AAAI.
128. [Customizing student networks from heterogeneous teachers via adaptive knowledge amalgamation](https://arxiv.org/pdf/1908.07121.pdf). Shen, C., Xue, M., Wang, X., Song, J., Sun, L., & Song, M. (2019b). ICCV.
129. [In teacher we trust: Learning compressed models for pedestrian detection](https://arxiv.org/pdf/1612.00478.pdf). Shen, J., Vesdapunt, N., Boddeti, V. N. & Kitani, K. M. (2016). 
130. [Feature representation of short utterances based on knowledge distillation for spoken language identification](https://isca-speech.org/archive/Interspeech_2018/pdfs/1519.pdf). Shen, P., Lu, X., Li, S. & Kawai, H. (2018). Interspeech.
131. [Knowledge Distillation-Based Representation Learning for Short-Utterance Spoken Language Identification](https://ieeexplore.ieee.org/document/9195801). Shen, P., Lu, X., Li, S., & Kawai, H. (2020). IEEE/ACM T AUDIO SPE 28: 2674-2683.
132. [Interactive learning of teacher-student model for short utterance spoken language identification](https://ieeexplore.ieee.org/document/8683371). Shen, P., Lu, X., Li, S. & Kawai, H. (2019c). ICASSP.
133. [Meal: Multi-model ensemble via adversarial learning](https://arxiv.org/pdf/1812.02425.pdf). Shen, Z., He, Z. & Xue, X. (2019d). AAAI.
134. [Compression of acoustic event detection models with quantized distillation](https://arxiv.org/pdf/1907.00873.pdf). Shi, B., Sun, M., Kao, C. C., Rozgic, V., Matsoukas, S. & Wang, C. (2019a). Interspeech.
135. [Semi-supervised acoustic event detection based on tri-training](https://arxiv.org/pdf/1904.12926.pdf). Shi, B., Sun, M., Kao, CC., Rozgic, V., Matsoukas, S. & Wang, C. (2019b). ICASSP.
136. [Knowledge distillation for recurrent neural network language modeling with trust regularization](https://arxiv.org/pdf/1904.04163.pdf). Shi, Y., Hwang, M. Y., Lei, X. & Sheng, H. (2019c). ICASSP.
137. [Empirical analysis of knowledge distillation technique for optimization of quantized deep neural networks](https://arxiv.org/pdf/1909.01688.pdf). Shin, S., Boo, Y. & Sung,W. (2019). 
138. [Incremental learning of object detectors without catastrophic forgetting](https://arxiv.org/pdf/1708.06977.pdf). Shmelkov, K., Schmid, C. & Alahari, K. (2017). ICCV.
139. [Knowledge squeezed adversarial network compression](https://arxiv.org/pdf/1904.05100.pdf). Shu, C., Li, P., Xie, Y., Qu, Y., Dai, L., & Ma, L.(2019).  
140. [Video object segmentation using teacher-student adaptation in a human robot interaction (hri) setting](https://arxiv.org/pdf/1810.07733.pdf). Siam, M., Jiang, C., Lu, S., Petrich, L., Gamal, M., Elhoseiny, M. & et al. (2019). ICRA.
141. [Structured transforms for small-footprint deep learning](https://arxiv.org/pdf/1510.01722.pdf). Sindhwani, V., Sainath, T. & Kumar, S. (2015). NeurIPS.
142. [Mastering the game of Go with deep neural networks and tree search](https://www.nature.com/articles/nature16961). Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre,
L., Van Den Driessche, G., ... & Dieleman, S. (2016). Nature, 529(7587): 484–489.
143. [Neural compatibility modeling with attentive knowledge distillation](https://arxiv.org/pdf/1805.00313.pdf). Song, X., Feng, F., Han, X., Yang, X., Liu,W. & Nie, L. (2018). SIGIR.
144. [Knowledge transfer with jacobian matching](https://arxiv.org/pdf/1803.00443.pdf). Srinivas, S. & Fleuret, F. (2018). ICML.
145. [Adapting models to signal degradation using distillation](https://arxiv.org/pdf/1604.00433.pdf). Su, J. C. & Maji, S. (2017). BMVC.
146. [Collaborative Teacher-Student Learning via Multiple Knowledge Transfer](https://arxiv.org/pdf/2101.08471.pdf). Sun, L., Gou, J., Du, L., & Tao, D. (2021) 
147. [Patient knowledge distillation for bert model compression](https://arxiv.org/pdf/1908.09355.pdf). Sun, S., Cheng, Y., Gan, Z. & Liu, J. (2019). NEMNLP-IJCNLP.
148. [Optimizing network performance for distributed dnn training on gpu clusters: Imagenet/alexnet training in 1.5 minutes](https://arxiv.org/pdf/1902.06855.pdf). Sun, P., Feng, W., Han, R., Yan, S., & Wen, Y. (2019).
149. [An investigation of a knowledge distillation method for ctc acoustic models](https://ieeexplore.ieee.org/document/8461995). Takashima, R., Li, S. & Kawai, H. (2018). ICASSP.
150. [Knowledge-Transfer Generative Adversarial Network for Text-to-Image Synthesis](https://ieeexplore.ieee.org/document/9210842). Tan, H., Liu, X., Liu, M., Yin, B., & Li, X. (2021). KTGAN:. IEEE TIP 30: 1275-1290.
151. [Mnasnet: Platform-aware neural architecture search for mobile](https://arxiv.org/pdf/1807.11626.pdf). Tan, M., Chen, B., Pang, R., Vasudevan, V., Sandler, M., Howard, A., & Le, Q. V. (2019). CVPR.
152. [EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/pdf/1905.11946.pdf). Tan, M., & Le, Q. (2019). ICML.
153. [Multilingual neural machine translation with knowledge distillation](https://arxiv.org/pdf/1902.10461.pdf). Tan, X., Ren, Y., He, D., Qin, T., Zhao, Z. & Liu, T. Y.
(2019). ICLR.
154. [Understanding and Improving Knowledge Distillation](https://arxiv.org/pdf/2002.03532.pdf). Tang, J., Shivanna, R., Zhao, Z., Lin, D., Singh, A., Chi, E. H., & Jain, S. (2020). 
155. [Ranking distillation: Learning compact ranking models with high performance for recommender system](https://arxiv.org/pdf/1809.07428.pdf). Tang, J. & Wang, K. (2018). SIGKDD.
156. [Distilling task-specific knowledge from bert into simple neural networks](https://arxiv.org/pdf/1903.12136.pdf). Tang, R., Lu, Y., Liu, L., Mou, L., Vechtomova, O.
& Lin, J. (2019). 
157. [Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results](https://arxiv.org/pdf/1703.01780.pdf). Tarvainen, A., & Valpola, H. (2017). NeurIPS.
158. [Cross-modal knowledge distillation for action recognition](https://arxiv.org/pdf/1910.04641.pdf). Thoker, F. M. & Gall, J. (2019). ICIP.
159. [Contrastive representation distillation](https://arxiv.org/pdf/1910.10699.pdf). Tian, Y., Krishnan, D. & Isola, P. (2020). ICLR.
160. [Understanding Generalization in Recurrent Neural Networks](https://openreview.net/pdf?id=rkgg6xBYDH). Tu, Z., He, F., & Tao, D. (2020). ICLR.
161. [Similarity-preserving knowledge distillation](https://arxiv.org/pdf/1907.09682.pdf). Tung, F. & Mori, G. (2019). ICCV.
162. [Well-read students learn better: The impact of student initialization on knowledge distillation](https://arxiv.org/pdf/1908.08962.pdf). Turc, I., Chang, M. W., Lee, K. & Toutanova, K.(2019). 
163. [Access to unlabeled data can speed up prediction time](https://www.cs.huji.ac.il/~shais/papers/UrnerBendavidShalev11.pdf). Urner, R., Shalev-Shwartz, S., Ben-David, S. (2011). ICML.
164. [ Do deep convolutional nets really need to be deep and convolutional?](https://arxiv.org/pdf/1603.05691.pdf). Urban, G., Geras, K. J., Kahou, S. E., Aslan, O., Wang, S., Caruana, R. & et al. (2017). ICLR.
165. [Learning using privileged information: similarity control and knowledge transfer](https://jmlr.org/papers/volume16/vapnik15b/vapnik15b.pdf). Vapnik, V. & Izmailov, R. (2015). J Mach Learn Res 16(1): 2023-2049.
166. [Unifying heterogeneous classifiers with distillation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Vongkulbhisal_Unifying_Heterogeneous_Classifiers_With_Distillation_CVPR_2019_paper.pdf). Vongkulbhisal, J., Vinayavekhin, P. & Visentini-Scarzanella, M. (2019). CVPR.
167. [Online Ensemble Model Compression using Knowledge Distillation](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123640018.pdf). Walawalkar, D., Shen, Z., & Savvides, M. (2020). ECCV.
168. [Model distillation with knowledge transfer from face classification to alignment and verification](https://arxiv.org/pdf/1709.02929.pdf). Wang, C., Lan, X. & Zhang, Y. (2017). 
169. [Knowledge distillation and student-teacher learning for visual intelligence: A review and new outlooks](https://arxiv.org/pdf/2004.05937.pdf). Wang, L., & Yoon, K. J. (2020). 
170. [Progressive blockwise knowledge distillation for neural network acceleration](https://www.ijcai.org/Proceedings/2018/0384.pdf). Wang, H., Zhao, H., Li, X. & Tan, X. (2018a). IJCAI.
171. [Private model compression via knowledge distillation](https://arxiv.org/pdf/1811.05072.pdf). Wang, J., Bao, W., Sun, L., Zhu, X., Cao, B. & Philip, SY. (2019a). AAAI.
172. [Deepvid: Deep visual interpretation and diagnosis for image classifiers via knowledge distillation](https://pubmed.ncbi.nlm.nih.gov/30892211/). Wang, J., Gou, L., Zhang, W., Yang, H. & Shen, H. W. (2019b).  TVCG 25(6): 2168-2180
173. [Discover the effective strategy for face recognition model compression by improved knowledge distillation](https://ieeexplore.ieee.org/document/8451808). Wang, M., Liu, R., Abe, N., Uchida, H., Matsunami, T. & Yamada, S. (2018b). ICIP.
174. [Improved knowledge distillation for training fast low resolution face recognition model](https://openaccess.thecvf.com/content_ICCVW_2019/papers/LSR/Wang_Improved_Knowledge_Distillation_for_Training_Fast_Low_Resolution_Face_Recognition_ICCVW_2019_paper.pdf). Wang, M., Liu, R., Hajime, N., Narishige, A., Uchida, H. & Matsunami, T.(2019c). ICCVW.
175. [Distilling Object Detectors with Fine-grained Feature Imitation](https://arxiv.org/pdf/1906.03609.pdf). Wang, T., Yuan, L., Zhang, X. & Feng, J. (2019d). CVPR.
176. [Dataset distillation](https://arxiv.org/pdf/1811.10959.pdf). Wang, T., Zhu, J. Y., Torralba, A., & Efros, A. A. (2018c). 
177. [Minilm: Deep self-attention distillation for task-agnostic compression of pretrained transformers](https://arxiv.org/pdf/2002.10957.pdf). Wang, W., Wei, F., Dong, L., Bao, H., Yang, N., & Zhou, M. (2020a). NeurIPS.
178. [A teacher-student framework for maintainable dialog manager](https://aclanthology.org/D18-1415.pdf). Wang, W., Zhang, J., Zhang, H., Hwang, M. Y., Zong, C. & Li, Z. (2018d). EMNLP.
179. [ Exclusivity-Consistency Regularized Knowledge Distillation for Face Recognition](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123690324.pdf). Wang, X., Fu, T., Liao, S., Wang, S., Lei, Z., & Mei, T. (2020b). ECCV.
180. [Progressive teacher-student learning for early action prediction](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wang_Progressive_Teacher-Student_Learning_for_Early_Action_Prediction_CVPR_2019_paper.pdf). Wang, X., Hu, J. F., Lai, J. H., Zhang, J. & Zheng, W. S. (2019e). CVPR.
181. [ Kdgan: Knowledge distillation with generative adversarial networks](https://papers.nips.cc/paper/2018/file/019d385eb67632a7e958e23f24bd07d7-Paper.pdf). Wang, X., Zhang, R., Sun, Y. & Qi, J. (2018e). NeurIPS.
182. [Packing convolutional neural networks in the frequency domain](https://ieeexplore.ieee.org/document/8413170). Wang, Y., Xu, C., Xu, C. & Tao, D. (2019f). IEEE TPAMI 41(10): 2495–2510.
183. [Adversarial learning of portable student networks](https://research.ubtrobot.com/file/publications/Adversarial%20Learning%20of%20Portable%20Student%20Networks.pdf). Wang, Y., Xu, C., Xu, C. & Tao, D. (2018f). AAAI.
184. [Joint architecture and knowledge distillation in CNN for Chinese text recognition](https://arxiv.org/pdf/1912.07806.pdf). Wang, Z. R., & Du, J. (2021). Pattern Recognition 111: 107722.
185. [Student-teacher network learning with enhanced features](https://ieeexplore.ieee.org/document/7953163). Watanabe, S., Hori, T., Le Roux, J. & Hershey, J. R. (2017). ICASSP.
186. [Online distilling from checkpoints for neural machine translation](https://aclanthology.org/N19-1192.pdf). Wei, H. R., Huang, S., Wang, R., Dai, X. & Chen, J.
(2019). NAACL-HLT.
187. [Quantizationmimic: Towards very tiny cnn for object detection](https://arxiv.org/pdf/1805.02152.pdf). Wei, Y., Pan, X., Qin, H., Ouyang,W. & Yan, J. (2018). ECCV.
188. [Sequence studentteacher training of deep neural networks](https://www.repository.cam.ac.uk/bitstream/handle/1810/256846/Wong_et_al-2016-Interspeech-VoR.pdf;jsessionid=CC19D00619DC9DAD0E7BDB96AB7A1D64?sequence=4). Wong, J. H. & Gales, M. (2016). Interspeech.
189. [Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search](https://arxiv.org/pdf/1812.03443.pdf). Wu, B., Dai, X., Zhang, P., Wang, Y., Sun, F., Wu, Y., ... & Keutzer, K. (2019). CVPR.
190. [Distilled person re-identification: Towards a more scalable system](https://openaccess.thecvf.com/content_CVPR_2019/papers/Wu_Distilled_Person_Re-Identification_Towards_a_More_Scalable_System_CVPR_2019_paper.pdf). Wu, A., Zheng, W. S., Guo, X. & Lai, J. H. (2019a). CVPR.
191. [Peer Collaborative Learning for Online Knowledge Distillation](https://arxiv.org/pdf/2006.04147.pdf). Wu, G., & Gong, S. (2021). AAAI.
192. [Quantized convolutional neural networks for mobile devices](https://arxiv.org/pdf/1512.06473.pdf). Wu, J., Leng, C., Wang, Y., Hu, Q. & Cheng, J. (2016). CVPR.
193. [Multi-teacher knowledge distillation for compressed video action recognition on deep neural networks](https://www.sciencedirect.com/science/article/abs/pii/S1383762119305028). Wu, M. C., Chiu, C. T. & Wu, K. H. (2019b). ICASSP.
194. [Learning an evolutionary embedding via massive knowledge distillation](https://openreview.net/pdf?id=z4mkM1NonYc). Wu, X., He, R., Hu, Y., & Sun, Z. (2020). International Journal of Computer Vision, 1-18.
195. [Complete random forest based class noise filtering learning for improving the generalizability of classifiers](https://ieeexplore.ieee.org/document/8481535). Xia, S., Wang, G., Chen, Z., & Duan, Y. (2018). IEEE TKDE 31(11): 2063-2078.
196. [Training convolutional neural networks with cheap convolutions and online distillation](https://arxiv.org/pdf/1909.13063.pdf). Xie, J., Lin, S., Zhang, Y. & Luo, L. (2019).
197. [Self-training with Noisy Student improves ImageNet classification](https://arxiv.org/pdf/1911.04252.pdf). Xie, Q., Hovy, E., Luong, M. T., & Le, Q. V. (2020). CVPR.
198. [Knowledge Distillation Meets Self-Supervision](https://arxiv.org/pdf/2006.07114.pdf). Xu, G., Liu, Z., Li, X., & Loy, C. C. (2020a). ECCV.
199. [Feature Normalized Knowledge Distillation for Image Classification](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700664.pdf). Xu, K., Rui, L., Li, Y., & Gu, L. (2020b). ECCV.
200. [Knowledge Transfer in Multi-Task Deep Reinforcement Learning for Continuous Control](https://arxiv.org/pdf/2010.07494.pdf). Xu, Z., Wu, K., Che, Z., Tang, J., & Ye, J.
(2020c). NeurIPS.
201. [Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks](https://arxiv.org/pdf/1709.00513.pdf). Xu, Z., Hsu, Y. C. & Huang, J. (2018a). ICLR Workshop.
202. [Data-distortion guided self-distillation for deep neural networks](https://ojs.aaai.org/index.php/AAAI/article/view/4498/4376). Xu, Z., Hsu, Y. C. & Huang, J. (2018b). BMVC.
203. [Data-distortion guided self-distillation for deep neural networks](https://ojs.aaai.org/index.php/AAAI/article/view/4498/4376). Xu, T. B., & Liu, C. L. (2019). AAAI.
204. [Vargfacenet: An efficient variable group convolutional neural network for lightweight face recognition](https://arxiv.org/pdf/1910.04985.pdf). Yan, M., Zhao, M., Xu, Z., Zhang, Q., Wang, G. & Su, Z. (2019). ICCVW.
205. [Knowledge distillation in generations: More tolerant teachers educate better students](https://arxiv.org/pdf/1805.05551.pdf). Yang, C., Xie, L., Qiao, S. & Yuille, A. (2019a). AAAI.
206. [Snapshot distillation: Teacher-student optimization in one generation](https://arxiv.org/pdf/1812.00123.pdf). Yang, C., Xie, L., Su, C. & Yuille, A. L. (2019b). CVPR.
207. [Knowledge distillation via adaptive instance normalization](https://arxiv.org/pdf/2003.04289.pdf). Yang, J., Martinez, B., Bulat, A., & Tzimiropoulos, G. (2020a). ECCV.
208. [Distilling Knowledge From Graph Convolutional Networks](https://arxiv.org/pdf/2003.10477.pdf). Yang, Y., Qiu, J., Song, M., Tao, D. & Wang, X. (2020b). CVPR.
209. [TextBrewer: An Open-Source Knowledge Distillation Toolkit for Natural Language Processing](https://arxiv.org/pdf/2002.12620.pdf). Yang, Z., Cui, Y., Chen, Z., Che, W., Liu, T., Wang, S., & Hu, G. (2020c). ACL.
210. [Model compression with two-stage multiteacher knowledge distillation for web question answering system](https://arxiv.org/pdf/1910.08381.pdf). Yang, Z., Shou, L., Gong, M., Lin, W. & Jiang, D. (2020d). WSDM.
211. [Knowledge Transfer via Dense Cross-Layer Mutual-Distillation](https://arxiv.org/pdf/2008.07816.pdf). Yao, A., & Sun, D. (2020). ECCV.
212. [Graph Few-shot Learning via Knowledge Transfer](https://arxiv.org/pdf/1910.03053.pdf). Yao, H., Zhang, C., Wei, Y., Jiang, M., Wang, S., Huang, J., Chawla, N. V., & Li, Z. (2020). AAAI.
213. [Data-Free Knowledge Amalgamation via Group-Stack Dual-GAN](https://arxiv.org/pdf/2003.09088.pdf). Ye, J., Ji, Y., Wang, X., Gao, X., & Song, M. (2020). CVPR.
214. [Student becoming the master: Knowledge amalgamation for joint scene parsing, depth estimation, and more](https://arxiv.org/pdf/1904.10167.pdf). Ye, J., Ji, Y., Wang, X., Ou, K., Tao, D. & Song, M. (2019). CVPR.
215. [A gift from knowledge distillation: Fast optimization, network minimization and transfer learning](https://openaccess.thecvf.com/content_cvpr_2017/papers/Yim_A_Gift_From_CVPR_2017_paper.pdf). Yim, J., Joo, D., Bae, J. & Kim, J. (2017). CVPR.
216. [Dreaming to distill: Data-free knowledge transfer via DeepInversion](https://arxiv.org/pdf/1912.08795.pdf). Yin, H., Molchanov, P., Alvarez, J. M., Li, Z., Mallya,
A., Hoiem, D., Jha, Niraj K., & Kautz, J. (2020). CVPR.
217. [Knowledge extraction with no observable data](https://openreview.net/pdf?id=BJzo8EBeIB). Yoo, J., Cho, M., Kim, T., & Kang, U. (2019). NeurIPS.
218. [Learning from multiple teacher networks](http://library.usc.edu.ph/ACM/KKD%202017/pdfs/p1285.pdf). You, S., Xu, C., Xu, C. & Tao, D. (2017). SIGKDD.
219. [Learning with single-teacher multi-student](https://ojs.aaai.org/index.php/AAAI/article/view/11636/11495). You, S., Xu, C., Xu, C. & Tao, D. (2018). AAAI.
220. [Large batch optimization for deep learning: Training bert in 76 minutes](https://arxiv.org/pdf/1904.00962.pdf). You, Y., Li, J., Reddi, S., Hseu, J., Kumar, S., Bhojanapalli, S., ... & Hsieh, C. J. (2019). ICLR.
221. [Learning metrics from teachers: Compact networks for image embedding](https://arxiv.org/pdf/1904.03624.pdf). Yu, L., Yazici, V. O., Liu, X., Weijer, J., Cheng, Y. &
Ramisa, A. (2019). CVPR.
222. [On compressing deep models by low rank and sparse decomposition](https://tongliang-liu.github.io/papers/CVPR2017compression.pdf). Yu, X., Liu, T., Wang, X., & Tao, D. (2017). CVPR.
223. [Reinforced Multi-Teacher Selection for Knowledge Distillation](https://arxiv.org/pdf/2012.06048.pdf). Yuan, F., Shou, L., Pei, J., Lin,W., Gong,M., Fu, Y., & Jiang, D. (2021). AAAI.
224. [Revisit knowledge distillation: a teacher-free framework](https://arxiv.org/pdf/1909.11723.pdf). Yuan, L., Tay, F. E., Li, G., Wang, T. & Feng, J. (2020). CVPR.
225. [CKD: Cross-task knowledge distillation for text-to-image synthesis](https://ieeexplore.ieee.org/document/8890866). Yuan, M., & Peng, Y. (2020). IEEE TMM 22(8): 1955-1968.
226. [Matching Guided Distillation](https://arxiv.org/pdf/2008.09958.pdf). Yue, K., Deng, J., & Zhou, F. (2020). ECCV.
227. [Regularizing Class-wise Predictions via Self-knowledge Distillation](https://arxiv.org/pdf/2003.13964.pdf). Yun, S., Park, J., Lee, K. & Shin, J. (2020). CVPR.
228. [Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer](https://arxiv.org/pdf/1612.03928.pdf). Zagoruyko, S. & Komodakis, N. (2017). ICLR.
229. [Lifelong gan: Continual learning for conditional image generation](https://arxiv.org/pdf/1907.10107.pdf). Zhai, M., Chen, L., Tung, F., He, J., Nawhal, M. & Mori, G. (2019). ICCV.
230. [Doubly convolutional neural networks](https://arxiv.org/pdf/1610.09716.pdf). Zhai, S., Cheng, Y., Zhang, Z. M. & Lu, W. (2016). NeurIPS.
231. [Robust Domain Randomised Reinforcement Learning through Peerto-Peer Distillation](https://arxiv.org/pdf/2012.04839.pdf). Zhao, C., & Hospedales, T. (2020).  NeurIPS.
232. [Highlight every step: Knowledge distillation via collaborative teaching](https://arxiv.org/pdf/1907.09643.pdf). Zhao, H., Sun, X., Dong, J., Chen, C., & Dong, Z. (2020a). . IEEE TCYB. 
233. [Knowledge as Priors: Cross-Modal Knowledge Generalization for Datasets without Superior Knowledge](https://arxiv.org/pdf/2004.00176.pdf). Zhao, L., Peng, X., Chen, Y., Kapadia, M., & Metaxas, D. N. (2020b). 
234. [Throughwall human pose estimation using radio signals](https://openaccess.thecvf.com/content_cvpr_2018/papers/Zhao_Through-Wall_Human_Pose_CVPR_2018_paper.pdf). Zhao, M., Li, T., Abu Alsh]eikh, M., Tian, Y., Zhao, H., Torralba, A. & Katabi, D. (2018). CVPR.
235. [Better and faster: knowledge transfer from multiple self-supervised learning tasks via graph distillation for video classification](https://arxiv.org/pdf/1804.10069.pdf). Zhang, C. & Peng, Y. (2018). IJCAI.
236. [Fast human pose estimation](https://openaccess.thecvf.com/content_CVPR_2019/papers/Zhang_Fast_Human_Pose_Estimation_CVPR_2019_paper.pdf). Zhang, F., Zhu, X. & Ye, M. (2019a). CVPR.
237. [An informationtheoretic view for deep learning](https://arxiv.org/pdf/1804.09060.pdf). Zhang, J., Liu, T., & Tao, D. (2018). 
238. [Adversarial co-distillation learning for image recognition](https://www.sciencedirect.com/science/article/abs/pii/S0031320320304623). Zhang, H., Hu, Z., Qin, W., Xu, M., & Wang, M. (2021a). Pattern Recognition 111: 107659.
239. [Task-Oriented Feature Distillation](https://papers.nips.cc/paper/2020/file/a96b65a721e561e1e3de768ac819ffbb-Paper.pdf). Zhang, L., Shi, Y., Shi, Z., Ma, K., & Bao, C. (2020a). NeurIPS.
240. [Be your own teacher: Improve the performance of convolutional neural networks via self distillation](https://arxiv.org/pdf/1905.08094.pdf). Zhang, L., Song, J., Gao, A., Chen, J., Bao, C. & Ma, K. (2019b). ICCV.
241. [Discriminability distillation in group representation learning](https://arxiv.org/pdf/2008.10850.pdf). Zhang, M., Song, G., Zhou, H., & Liu, Y. (2020b). ECCV.
242. [Future-Guided Incremental Transformer for Simultaneous Translation](https://arxiv.org/pdf/2012.12465.pdf). Zhang, S., Feng, Y., & Li, L. (2021b). AAAI.
243. [Knowledge Integration Networks for Action Recognition](https://arxiv.org/pdf/2002.07471.pdf). Zhang, S., Guo, S., Wang, L., Huang, W., & Scott, M. R. (2020c). AAAI.
244. [Reliable Data Distillation on Graph Convolutional Network](https://dl.acm.org/doi/10.1145/3318464.3389706). Zhang, W., Miao, X., Shao, Y., Jiang, J., Chen,
L., Ruas, O., & Cui, B. (2020d). ACM SIGMOD.
245. [Diverse Knowledge Distillation for End-to-End Person Search](https://arxiv.org/pdf/2012.11187.pdf). Zhang, X., Wang, X., Bian, J. W., Shen, C., & You, M.
(2021c). AAAI.
246. [Shufflenet: An extremely efficient convolutional neural network for mobile devices](https://arxiv.org/pdf/1707.01083.pdf). Zhang, X., Zhou, X., Lin, M. & Sun, J. (2018a). CVPR.
247. [Prime-Aware Adaptive Distillation](https://arxiv.org/pdf/2008.01458.pdf). Zhang, Y., Lan, Z., Dai, Y., Zeng, F., Bai, Y., Chang, J., &Wei, Y. (2020e). ECCV.
248. [Deep mutual learning](https://arxiv.org/pdf/1706.00384.pdf). Zhang, Y., Xiang, T., Hospedales, T. M. & Lu, H. (2018b). CVPR.
249. [Self-Distillation as Instance-Specific Label Smoothing](https://arxiv.org/pdf/2006.05065.pdf). Zhang, Z., & Sabuncu, M. R. (2020). NeurIPS.
250. [Object Relational Graph with Teacher-Recommended Learning for Video Captioning](https://arxiv.org/pdf/2002.11566.pdf). Zhang, Z., Shi, Y., Yuan, C., Li, B., Wang, P.,
Hu, W., & Zha, Z. J. (2020f). CVPR.
251. [Understanding knowledge distillation in non-autoregressive machine translation](https://arxiv.org/pdf/1911.02727.pdf). Zhou C, Neubig G, Gu J (2019a). ICLR.
252. [Rocket launching: A universal and efficient framework for training well-performing light net](https://arxiv.org/pdf/1708.04106.pdf). Zhou, G., Fan, Y., Cui, R., Bian, W., Zhu, X. & Gai, K. (2018). AAAI.
253. [Two-stage image classification supervised by a single teacher single student model](https://arxiv.org/ftp/arxiv/papers/1909/1909.12111.pdf). Zhou, J., Zeng, S. & Zhang, B. (2019b). BMVC.
254. [M2KD: Incremental Learning via Multi-model and Multi-level Knowledge Distillation](https://arxiv.org/pdf/1904.01769.pdf). Zhou, P., Mai, L., Zhang, J., Xu, N., Wu, Z. & Davis, L. S. (2020). BMVC.
255. [Low-resolution visual recognition via deep feature distillation](https://ieeexplore.ieee.org/document/8682926). Zhu,M., Han, K., Zhang, C., Lin, J. &Wang, Y. (2019). ICASSP.
